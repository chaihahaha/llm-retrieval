import os
import sys
import json
import requests
from pathlib import Path
import argparse
from langchain_text_splitters import RecursiveCharacterTextSplitter

# æ–‡æ¡£æ–‡æœ¬æå–å‡½æ•°
def extract_text_from_file(filepath, supported_exts, keep_chunks=False):
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åï¼Œè°ƒç”¨å¯¹åº”åº“æå–æ–‡æœ¬"""
    filepath = Path(filepath)
    ext = filepath.suffix.lower()

    try:
        if ext == ".pdf":
            import PyPDF2
            with open(filepath, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                chunks = []
                for page in reader.pages:
                    chunks.append(page.extract_text().strip())
                if keep_chunks:
                    return chunks
                else:
                    return "\n".join(chunks)
        elif ext in [".html", ".htm"]:
            from bs4 import BeautifulSoup
            with open(filepath, "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f.read(), "lxml")
                # ç§»é™¤ script å’Œ style
                for script in soup(["script", "style"]):
                    script.decompose()
                text = soup.get_text(separator="\n")
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                if keep_chunks:
                    return list(chunks)
                else:
                    text = "\n".join(chunk for chunk in chunks if chunk)
                    return text
        elif ext in [".doc", ".docx"]:
            from docx import Document
            doc = Document(filepath)
            chunks = [para.text for para in doc.paragraphs]
            if keep_chunks:
                return list(chunks)
            else:
                text = "\n".join(chunks)
                return text.strip()
        elif ext in supported_exts:
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read().strip()
                if keep_chunks:
                    return [text]
                else:
                    return text
        else:
            print(f"âš ï¸ æœªçŸ¥æ–‡ä»¶ç±»å‹: {filepath.name}ï¼Œè·³è¿‡")
            if keep_chunks:
                return []
            else:
                return ""
    except Exception as e:
        print(f"âŒ è¯»å– {filepath} å¤±è´¥: {e}")
        if keep_chunks:
            return []
        else:
            return ""

def subdivide_chunks(chunks, text_splitter):
    documents_chunks = []
    documents_chunks_filename = []
    for doc_index,doc in enumerate(chunks):
        new_chunks = text_splitter.split_text(doc)
        documents_chunks += new_chunks
    return documents_chunks

def main():
    parser = argparse.ArgumentParser(description="Rerank documents using local API")
    parser.add_argument("--docs_dir", required=True, help="ç›®å½•è·¯å¾„ï¼ŒåŒ…å«PDFã€HTMLã€TXTã€DOCXç­‰æ–‡æ¡£")
    query_group = parser.add_mutually_exclusive_group(required=True)
    query_group.add_argument("--query_file", type=str, help="åŒ…å«æŸ¥è¯¢è¯­å¥çš„æ–‡æœ¬æ–‡ä»¶")
    query_group.add_argument("--query", type=str, help="æŸ¥è¯¢è¯­å¥å­—ç¬¦ä¸²")
    parser.add_argument("--add_ext", default=".py.cpp.c.rs", required=False, help="é¢å¤–çš„æ–‡æœ¬æ ¼å¼æ–‡ä»¶åç¼€")
    parser.add_argument("--top_n", type=int, default=15, required=False, help="æœ€ä½³åŒ¹é…ç»“æœæ˜¾ç¤ºæ•°é‡")
    parser.add_argument("--chunk_lines", type=int, default=2, required=False, help="æœç´¢æœ€å°å•å…ƒä¸ºå¤šå°‘è¡Œ")

    args = parser.parse_args()

    # 1. æ·»åŠ é¢å¤–çš„æ–‡æœ¬æ–‡ä»¶åç¼€
    supported_exts = {".pdf", ".html", ".htm", ".txt", ".docx", ".doc"}
    for ext in args.add_ext.split("."):
        if ext:
            supported_exts.add(f".{ext}")


    # 2. è¯»å–æŸ¥è¯¢è¯­å¥
    if args.query:
        query = args.query
    elif args.query_file:
        query = extract_text_from_file(args.query_file, supported_exts)
    else:
        query = ""

    if not query:
        print("âŒ æŸ¥è¯¢æ–‡ä»¶ä¸ºç©º")
        sys.exit(1)
    else:
        print(f"ç”¨æˆ·æŸ¥è¯¢: {query}")

    # 3. éå† docs_dir ä¸‹çš„æ‰€æœ‰æ”¯æŒæ–‡æ¡£
    docs_dir = Path(args.docs_dir)
    
    if not docs_dir.exists() or not docs_dir.is_dir():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {docs_dir}")
        sys.exit(1)

    documents = []
    documents_chunks = []
    chunk2filename_idx = []
    documents_paths = []
    fn_idx = 0
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

    for ext in supported_exts:
        for filepath in docs_dir.rglob(f"*{ext}"):
            if filepath.is_file() and filepath.suffix.lower() in supported_exts:
                documents_paths.append(filepath.name)

                new_chunks = extract_text_from_file(filepath, supported_exts, keep_chunks=True)
                if new_chunks:
                    documents.append("\n".join(new_chunks))
                    new_chunks = subdivide_chunks(new_chunks, text_splitter)
                    documents_chunks += new_chunks
                    chunk2filename_idx += [fn_idx] * len(new_chunks)

                fn_idx += 1

    if not documents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡æ¡£ï¼ˆæ”¯æŒï¼šPDFã€HTMLã€TXTã€DOCXï¼‰")
        sys.exit(1)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")


    # 4. æ„é€ è¯·æ±‚
    URL = "http://127.0.0.1:5678"
    payload = {
        "model": "M",
        "query": query,
        "texts": True,
        "return_text": True,
        "top_n": args.top_n,
        "documents": documents_chunks
    }

    # 5. å‘é€è¯·æ±‚
    try:
        response = requests.post(
            f"{URL}/v1/rerank",
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload)
        )
        
        # æ‰“å°çŠ¶æ€
        print(f"\nğŸ“Š Status Code: {response.status_code}")

        # è§£æå“åº”
        response_json = response.json()
        
        if "results" not in response_json:
            #print(json.dumps(response_json, indent=4, ensure_ascii=False))
            results = response_json
        else:
            results = response_json["results"]
        
        # 6. æŒ‰ score æ’åºï¼ˆAPIè¿”å›çš„å·²ç»æ˜¯æ’åºå¥½çš„ï¼Œä½†ä¿é™©èµ·è§ï¼‰
        results.sort(key=lambda x: x.get("score", 0), reverse=True)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ† Reranked Results (Top {len(results)}):")
        print("-" * 80)
        
        for idx, result in enumerate(results[:args.top_n], start=1):  # åªæ˜¾ç¤ºå‰15ä¸ª
            chunk_index = result.get("index", -1)
            score = result.get("score", 0.0)

            if chunk_index < len(chunk2filename_idx):
                filename = documents_paths[chunk2filename_idx[chunk_index]]
            else:
                filename = f"[æœªçŸ¥æ–‡æ¡£_{chunk_index}]"

            print(f"{idx:2d}. {filename} (score: {score})")
            print(f"    å†…å®¹: {documents_chunks[chunk_index]}")
            print()

        # å¯é€‰ï¼šè¾“å‡ºå®Œæ•´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"\nğŸ” å®Œæ•´å“åº”:")
        print(json.dumps(response_json, indent=4, ensure_ascii=False))

    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        sys.exit(1)
    except json.JSONDecodeError:
        print("âŒ å“åº”ä¸æ˜¯åˆæ³• JSON:")
        print(response.text)
        sys.exit(1)

if __name__ == "__main__":
    main()
