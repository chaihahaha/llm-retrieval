import os
import sys
import json
import requests
from pathlib import Path
import argparse

# æ–‡æ¡£æ–‡æœ¬æå–å‡½æ•°
def extract_text_from_file(filepath):
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åï¼Œè°ƒç”¨å¯¹åº”åº“æå–æ–‡æœ¬"""
    filepath = Path(filepath)
    ext = filepath.suffix.lower()

    try:
        if ext == ".pdf":
            import PyPDF2
            with open(filepath, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text.strip()
        elif ext == ".html" or ext == ".htm":
            from bs4 import BeautifulSoup
            with open(filepath, "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f.read(), "lxml")
                # ç§»é™¤ script å’Œ style
                for script in soup(["script", "style"]):
                    script.decompose()
                text = soup.get_text(separator="\n")
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = "\n".join(chunk for chunk in chunks if chunk)
                return text
        elif ext == ".txt":
            with open(filepath, "r", encoding="utf-8") as f:
                return f.read().strip()
        elif ext == ".docx":
            from docx import Document
            doc = Document(filepath)
            text = "\n".join([para.text for para in doc.paragraphs])
            return text.strip()
        else:
            print(f"âš ï¸ æœªçŸ¥æ–‡ä»¶ç±»å‹: {filepath.name}ï¼Œè·³è¿‡")
            return ""
    except Exception as e:
        print(f"âŒ è¯»å– {filepath} å¤±è´¥: {e}")
        return ""

def main():
    parser = argparse.ArgumentParser(description="Rerank documents using local API")
    parser.add_argument("--docs_dir", required=True, help="ç›®å½•è·¯å¾„ï¼ŒåŒ…å«PDFã€HTMLã€TXTã€DOCXç­‰æ–‡æ¡£")
    parser.add_argument("--query_file", required=True, help="åŒ…å«æŸ¥è¯¢è¯­å¥çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆä¸€è¡Œä¸€ä¸ªæˆ–æ•´ä¸ªå†…å®¹ä½œä¸ºå•æ¡æŸ¥è¯¢ï¼‰")

    args = parser.parse_args()

    # 1. è¯»å–æŸ¥è¯¢è¯­å¥
    with open(args.query_file, "r", encoding="utf-8") as f:
        query = f.read().strip()
    if not query:
        print("âŒ æŸ¥è¯¢æ–‡ä»¶ä¸ºç©º")
        sys.exit(1)
    else:
        print(f"ç”¨æˆ·æŸ¥è¯¢: {query}")

    # 2. éå† docs_dir ä¸‹çš„æ‰€æœ‰æ”¯æŒæ–‡æ¡£
    supported_exts = {".pdf", ".html", ".htm", ".txt", ".docx"}
    docs_dir = Path(args.docs_dir)
    
    if not docs_dir.exists() or not docs_dir.is_dir():
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {docs_dir}")
        sys.exit(1)

    documents = []
    file_names = []

    for filepath in docs_dir.iterdir():
        if filepath.is_file() and filepath.suffix.lower() in supported_exts:
            text = extract_text_from_file(filepath)
            if text:  # åªä¿ç•™éç©ºæ–‡æœ¬
                documents.append(text)
                file_names.append(filepath.name)
    print('####################docs')
    print(documents)

    if not documents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡æ¡£ï¼ˆæ”¯æŒï¼šPDFã€HTMLã€TXTã€DOCXï¼‰")
        sys.exit(1)

    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

    # 3. æ„é€ è¯·æ±‚
    URL = "http://127.0.0.1:5678"
    payload = {
        "model": "M",
        "query": query,
        "texts": False,
        "return_text": False,  # ä¸è¿”å›åŸæ–‡ï¼Œåªè¿”å›æ’åº
        "top_n": len(documents),  # å¯è®¾ä¸ºè¾ƒå°å€¼ï¼Œå¦‚10
        "documents": documents
    }

    # 4. å‘é€è¯·æ±‚
    try:
        response = requests.post(
            f"{URL}/v1/rerank",
            headers={"Content-Type": "application/json"},
            data=json.dumps(payload)
        )
        
        # æ‰“å°çŠ¶æ€
        print(f"\nğŸ“Š Status Code: {response.status_code}")

        # è§£æå“åº”
        response_json = response.json()
        
        if "results" not in response_json:
            #print(json.dumps(response_json, indent=4, ensure_ascii=False))
            results = response_json
        else:
            results = response_json["results"]
        
        # 5. æŒ‰ relevance_score æ’åºï¼ˆAPIè¿”å›çš„å·²ç»æ˜¯æ’åºå¥½çš„ï¼Œä½†ä¿é™©èµ·è§ï¼‰
        #results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ† Reranked Results (Top {len(results)}):")
        print("-" * 80)
        
        for idx, result in enumerate(results[:15], start=1):  # åªæ˜¾ç¤ºå‰15ä¸ª
            doc_index = result.get("index", -1)
            score = result.get("relevance_score", 0.0)

            if doc_index < len(file_names):
                filename = file_names[doc_index]
            else:
                filename = f"[æœªçŸ¥æ–‡æ¡£_{doc_index}]"

            print(f"{idx:2d}. {filename} (score: {score})")
            #print(f"æ–‡ä»¶å†…å®¹:{documents[doc_index]}")

        # å¯é€‰ï¼šè¾“å‡ºå®Œæ•´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print(f"\nğŸ” å®Œæ•´å“åº”:")
        print(json.dumps(response_json, indent=4, ensure_ascii=False))

    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        sys.exit(1)
    except json.JSONDecodeError:
        print("âŒ å“åº”ä¸æ˜¯åˆæ³• JSON:")
        print(response.text)
        sys.exit(1)

if __name__ == "__main__":
    main()
